{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Food Classification from Images Using Convolutional Neural Networks</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Food Classification using keras\n",
    "<br/>\n",
    "<div style=\"text-align: justify\">\n",
    "Convolutional neural networks (CNN) have been widely used in automatic image classification systems. In most cases, features from the top layer of the CNN are utilized for classification; however, those features may not contain enough useful information to predict an image correctly. In some cases, features from the lower layer carry more discriminative power than those from the top. Therefore, applying features from a specific layer only to classification seems to be a process that does not utilize learned CNN’s potential discriminant power to its full extent. This inherent property leads to the need for fusion of features from multiple layers. To address this problem, we propose a method of combining features from multiple layers in given CNN models. Moreover, already learned CNN models with training images are reused to extract features from multiple layers. The proposed fusion method is evaluated according to image classification benchmark data sets, CIFAR-10, NORB, and SVHN. In all cases, we show that the proposed method improves the reported performances of the existing models by 0.38%, 3.22% and 0.13%, respectively.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks\n",
    "<br/>\n",
    "<div style=\"text-align: justify\">\n",
    "The Convolutional neural network(CNN) is a deep learning architecture that has numerous application in computer vision and natural language processing. The CNN classifies objects based on number of features matched.\n",
    "\n",
    "Steps involed in creating CNN\n",
    "    <ol>\n",
    "<li>Convolution</li>\n",
    "<li> Pooling</li>\n",
    "<li> Flattening</li>\n",
    "<li> Full Connection</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![Dataset](convo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li><b>Convolution</b>\n",
    "<br/>\n",
    "<div style=\"text-align: justify\">\n",
    "ConvNets derive their name from the “convolution” operator. The primary purpose of Convolution in case of a ConvNet is to extract features from the input image. Convolution preserves the spatial relationship between pixels by learning image features using small squares of input data. \n",
    "</div>\n",
    "    </li>\n",
    "<li> <b>Pooling</b>\n",
    "<br/>\n",
    "<div style=\"text-align: justify\">\n",
    "Spatial Pooling (also called subsampling or downsampling) reduces the dimensionality of each feature map but retains the most important information. Spatial Pooling can be of different types: Max, Average, Sum etc.\n",
    "In case of Max Pooling, we define a spatial neighborhood (for example, a 2×2 window) and take the largest element from the rectified feature map within that window. Instead of taking the largest element we could also take the average (Average Pooling) or sum of all elements in that window. In practice, Max Pooling has been shown to work better.\n",
    "</div>\n",
    "    </li>\n",
    "<li> <b>Flattening</b>\n",
    "<br/>\n",
    "<div style=\"text-align: justify\">\n",
    "Convert the 2D matrix to a column vector so that it can passed through an artificial neural network\n",
    "</div>\n",
    "    </li>\n",
    "<li> <b>Full Connection</b>\n",
    "<br/>\n",
    "<div style=\"text-align: justify\">\n",
    "The Fully Connected layer is a traditional Multi Layer Perceptron that uses a softmax activation function in the output layer (other classifiers like SVM can also be used, but will stick to softmax in this post). The term “Fully Connected” implies that every neuron in the previous layer is connected to every neuron on the next layer.\n",
    "The output from the convolutional and pooling layers represent high-level features of the input image. The purpose of the Fully Connected layer is to use these features for classifying the input image into various classes based on the training dataset. \n",
    "</div>\n",
    "    </li>\n",
    "   </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Food Classification using keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to import the libraries required by the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is located in folder named dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists of three classes, french_fries, pizza and samosa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"dataset.png\" alt = \"DataSet\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each folder contains approximately 1000 image files in each category. The name of the folder is actually the label of those files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the CNN\n",
    "classifier = Sequential()\n",
    "#Convolution and Max pooling\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (128, 128, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "classifier.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "classifier.add(Conv2D(128, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flatten\n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full connection\n",
    "classifier.add(Dense(128, activation = 'relu'))\n",
    "classifier.add(Dense(3, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile classifier\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2400 images belonging to 3 classes.\n",
      "Found 600 images belonging to 3 classes.\n",
      "Epoch 1/50\n",
      "25/25 [==============================] - 21s 840ms/step - loss: 1.1524 - acc: 0.3637 - val_loss: 1.0882 - val_acc: 0.4152\n",
      "Epoch 2/50\n",
      "25/25 [==============================] - 19s 776ms/step - loss: 1.0667 - acc: 0.4000 - val_loss: 1.0003 - val_acc: 0.4330\n",
      "Epoch 3/50\n",
      "25/25 [==============================] - 19s 755ms/step - loss: 0.9971 - acc: 0.4838 - val_loss: 0.9292 - val_acc: 0.5231\n",
      "Epoch 4/50\n",
      "25/25 [==============================] - 19s 768ms/step - loss: 0.9371 - acc: 0.5325 - val_loss: 0.8440 - val_acc: 0.5893\n",
      "Epoch 5/50\n",
      "25/25 [==============================] - 19s 770ms/step - loss: 0.9337 - acc: 0.5212 - val_loss: 0.8720 - val_acc: 0.5536\n",
      "Epoch 6/50\n",
      "25/25 [==============================] - 19s 760ms/step - loss: 0.8464 - acc: 0.5712 - val_loss: 0.9897 - val_acc: 0.4769\n",
      "Epoch 7/50\n",
      "25/25 [==============================] - 18s 740ms/step - loss: 0.9147 - acc: 0.5425 - val_loss: 1.0030 - val_acc: 0.4821\n",
      "Epoch 8/50\n",
      "25/25 [==============================] - 18s 728ms/step - loss: 0.8339 - acc: 0.6088 - val_loss: 0.8180 - val_acc: 0.5446\n",
      "Epoch 9/50\n",
      "25/25 [==============================] - 19s 768ms/step - loss: 0.7834 - acc: 0.6262 - val_loss: 0.9812 - val_acc: 0.5417\n",
      "Epoch 10/50\n",
      "25/25 [==============================] - 18s 733ms/step - loss: 0.7574 - acc: 0.6388 - val_loss: 0.8656 - val_acc: 0.6116\n",
      "Epoch 11/50\n",
      "25/25 [==============================] - 18s 713ms/step - loss: 0.7931 - acc: 0.6500 - val_loss: 1.1243 - val_acc: 0.4630\n",
      "Epoch 12/50\n",
      "25/25 [==============================] - 19s 763ms/step - loss: 0.7420 - acc: 0.6487 - val_loss: 0.6915 - val_acc: 0.7277\n",
      "Epoch 13/50\n",
      "25/25 [==============================] - 18s 732ms/step - loss: 0.7572 - acc: 0.6350 - val_loss: 0.8085 - val_acc: 0.6473\n",
      "Epoch 14/50\n",
      "25/25 [==============================] - 19s 741ms/step - loss: 0.6951 - acc: 0.7000 - val_loss: 0.8908 - val_acc: 0.5880\n",
      "Epoch 15/50\n",
      "25/25 [==============================] - 19s 745ms/step - loss: 0.7090 - acc: 0.6725 - val_loss: 0.7113 - val_acc: 0.6875\n",
      "Epoch 16/50\n",
      "25/25 [==============================] - 18s 726ms/step - loss: 0.6190 - acc: 0.7275 - val_loss: 0.6297 - val_acc: 0.7321\n",
      "Epoch 17/50\n",
      "25/25 [==============================] - 18s 738ms/step - loss: 0.6579 - acc: 0.7338 - val_loss: 0.6629 - val_acc: 0.6991\n",
      "Epoch 18/50\n",
      "25/25 [==============================] - 18s 732ms/step - loss: 0.6441 - acc: 0.7175 - val_loss: 0.6643 - val_acc: 0.7366\n",
      "Epoch 19/50\n",
      "25/25 [==============================] - 18s 711ms/step - loss: 0.6074 - acc: 0.7462 - val_loss: 0.5699 - val_acc: 0.7731\n",
      "Epoch 20/50\n",
      "25/25 [==============================] - 19s 751ms/step - loss: 0.5206 - acc: 0.7787 - val_loss: 0.6229 - val_acc: 0.7455\n",
      "Epoch 21/50\n",
      "25/25 [==============================] - 18s 724ms/step - loss: 0.5871 - acc: 0.7425 - val_loss: 0.6799 - val_acc: 0.7277\n",
      "Epoch 22/50\n",
      "25/25 [==============================] - 19s 762ms/step - loss: 0.6856 - acc: 0.6938 - val_loss: 0.6105 - val_acc: 0.7407\n",
      "Epoch 23/50\n",
      "25/25 [==============================] - 19s 747ms/step - loss: 0.5958 - acc: 0.7662 - val_loss: 0.5394 - val_acc: 0.7946\n",
      "Epoch 24/50\n",
      "25/25 [==============================] - 18s 713ms/step - loss: 0.5022 - acc: 0.7913 - val_loss: 0.7083 - val_acc: 0.7366\n",
      "Epoch 25/50\n",
      "25/25 [==============================] - 18s 708ms/step - loss: 0.4787 - acc: 0.8075 - val_loss: 0.5537 - val_acc: 0.7593\n",
      "Epoch 26/50\n",
      "25/25 [==============================] - 18s 733ms/step - loss: 0.5278 - acc: 0.7888 - val_loss: 0.5723 - val_acc: 0.7411\n",
      "Epoch 27/50\n",
      "25/25 [==============================] - 18s 709ms/step - loss: 0.5278 - acc: 0.7900 - val_loss: 0.5604 - val_acc: 0.7991\n",
      "Epoch 28/50\n",
      "25/25 [==============================] - 19s 742ms/step - loss: 0.4195 - acc: 0.8325 - val_loss: 0.5594 - val_acc: 0.8333\n",
      "Epoch 29/50\n",
      "25/25 [==============================] - 19s 751ms/step - loss: 0.5264 - acc: 0.7812 - val_loss: 0.4576 - val_acc: 0.8125\n",
      "Epoch 30/50\n",
      "25/25 [==============================] - 18s 726ms/step - loss: 0.4916 - acc: 0.8013 - val_loss: 0.6152 - val_acc: 0.7731\n",
      "Epoch 31/50\n",
      "25/25 [==============================] - 18s 730ms/step - loss: 0.4613 - acc: 0.8125 - val_loss: 0.4921 - val_acc: 0.7857\n",
      "Epoch 32/50\n",
      "25/25 [==============================] - 18s 720ms/step - loss: 0.4297 - acc: 0.8325 - val_loss: 0.5208 - val_acc: 0.8080\n",
      "Epoch 33/50\n",
      "25/25 [==============================] - 18s 724ms/step - loss: 0.4267 - acc: 0.8250 - val_loss: 0.3886 - val_acc: 0.8796\n",
      "Epoch 34/50\n",
      "25/25 [==============================] - 18s 738ms/step - loss: 0.4154 - acc: 0.8163 - val_loss: 0.5013 - val_acc: 0.8125\n",
      "Epoch 35/50\n",
      "25/25 [==============================] - 18s 722ms/step - loss: 0.4045 - acc: 0.8575 - val_loss: 0.5249 - val_acc: 0.8036\n",
      "Epoch 36/50\n",
      "25/25 [==============================] - 18s 736ms/step - loss: 0.4210 - acc: 0.8337 - val_loss: 0.4143 - val_acc: 0.8704\n",
      "Epoch 37/50\n",
      "25/25 [==============================] - 18s 725ms/step - loss: 0.3501 - acc: 0.8588 - val_loss: 0.6437 - val_acc: 0.7679\n",
      "Epoch 38/50\n",
      "25/25 [==============================] - 18s 722ms/step - loss: 0.4106 - acc: 0.8362 - val_loss: 0.4265 - val_acc: 0.8287\n",
      "Epoch 39/50\n",
      "25/25 [==============================] - 19s 748ms/step - loss: 0.3905 - acc: 0.8513 - val_loss: 0.6041 - val_acc: 0.8036\n",
      "Epoch 40/50\n",
      "25/25 [==============================] - 18s 728ms/step - loss: 0.3068 - acc: 0.8800 - val_loss: 0.4507 - val_acc: 0.8527\n",
      "Epoch 41/50\n",
      "25/25 [==============================] - 18s 740ms/step - loss: 0.3354 - acc: 0.8650 - val_loss: 0.3921 - val_acc: 0.8565\n",
      "Epoch 42/50\n",
      "25/25 [==============================] - 19s 761ms/step - loss: 0.3293 - acc: 0.8762 - val_loss: 0.4522 - val_acc: 0.8482\n",
      "Epoch 43/50\n",
      "25/25 [==============================] - 18s 727ms/step - loss: 0.2974 - acc: 0.8888 - val_loss: 0.5118 - val_acc: 0.8393\n",
      "Epoch 44/50\n",
      "25/25 [==============================] - 19s 745ms/step - loss: 0.3166 - acc: 0.8700 - val_loss: 0.3935 - val_acc: 0.8750\n",
      "Epoch 45/50\n",
      "25/25 [==============================] - 18s 740ms/step - loss: 0.3452 - acc: 0.8588 - val_loss: 0.6467 - val_acc: 0.7679\n",
      "Epoch 46/50\n",
      "25/25 [==============================] - 18s 717ms/step - loss: 0.3813 - acc: 0.8412 - val_loss: 0.6459 - val_acc: 0.7679\n",
      "Epoch 47/50\n",
      "25/25 [==============================] - 19s 754ms/step - loss: 0.3344 - acc: 0.8725 - val_loss: 0.4770 - val_acc: 0.8333\n",
      "Epoch 48/50\n",
      "25/25 [==============================] - 18s 730ms/step - loss: 0.2837 - acc: 0.8862 - val_loss: 0.3645 - val_acc: 0.8616\n",
      "Epoch 49/50\n",
      "25/25 [==============================] - 18s 722ms/step - loss: 0.2686 - acc: 0.8938 - val_loss: 0.7088 - val_acc: 0.7731\n",
      "Epoch 50/50\n",
      "25/25 [==============================] - 19s 751ms/step - loss: 0.3439 - acc: 0.8575 - val_loss: 0.5702 - val_acc: 0.7902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb33e7bcc0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting CNN to the images\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True) \n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "training_set = train_datagen.flow_from_directory('./dataset/training_set', target_size=(128, 128), batch_size=32, class_mode='categorical')\n",
    "test_set = test_datagen.flow_from_directory('./dataset/test_set', target_size=(128, 128), batch_size=32, class_mode='categorical')\n",
    "classifier.fit_generator(training_set, steps_per_epoch=800/32, epochs=50, validation_data=test_set, validation_steps = 200/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'french_fries', 1: 'pizza', 2: 'samosa'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = (training_set.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "import os\n",
    "target_dir = './models/'\n",
    "if not os.path.exists(target_dir):\n",
    "  os.mkdir(target_dir)\n",
    "classifier.save('./models/model.h5')\n",
    "classifier.save_weights('./models/weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting a new image from url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries for creating GUI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from keras.models import Sequential, load_model\n",
    "from PIL import Image, ImageTk\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from keras.preprocessing import image as image_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"justify\">\n",
    "    We now create a url and a button for browsing and loading image.\n",
    "The trained CNN is loaded and the saved parameters are restored.\n",
    "Prediction is made on the new image which is then output and \n",
    "the output is displayed on canvas.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "img_width, img_height = 128, 128\n",
    "model_path = './models/model.h5'\n",
    "model_weights_path = './models/weights.h5'\n",
    "model = load_model(model_path)\n",
    "model.load_weights(model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.kingarthurflour.com/sites/default/files/recipe_legacy/20-3-large.jpg\n",
      "Predicted answer : pizza\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.kingarthurflour.com/sites/default/files/recipe_legacy/20-3-large.jpg' # change to image url\n",
    "print(url)\n",
    "response = requests.get(url)\n",
    "test_image = Image.open(BytesIO(response.content))\n",
    "put_image = test_image.resize((400,400)) \n",
    "test_image = test_image.resize((128,128))  \n",
    "test_image = image_utils.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "result = model.predict_on_batch(test_image)\n",
    "\n",
    "if result[0][0] == 1:\n",
    "    ans = 'french fries'\n",
    "elif result[0][1] == 1:\n",
    "    ans = 'pizza'\n",
    "elif result[0][2] == 1:\n",
    "    ans = 'samosa'\n",
    "    \n",
    "print('Predicted answer : ' +  ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<div style=\"text-align: justify\">\n",
    "The performance of the system is high, and is considered\n",
    "acceptable from a usage point of view. However, the CNNs\n",
    "need high-performance computing machines in order to\n",
    "experiment on the huge multi-media datasets. The CNN\n",
    "is capable of train highly non-linear data, and for that in contrast, it takes more computational time to train the network.\n",
    "However, the performance matters a lot, and once the system\n",
    "is properly trained, the system can produce the results in less\n",
    "time. The images are properly preprocessed and all kinds of\n",
    "images are tested with CNN. From this, it is concluded that\n",
    "CNNs are more suitable for classifying the images when the\n",
    "number of classes are more.\n",
    "The task of image classification can be extended using\n",
    "prominent features that can categorize food images. Since\n",
    "the CNNs are consuming high computational time, the\n",
    "feature-based approach is highly appreciable. A multi-level\n",
    "classification approach (hierarchical approach) is suitable to\n",
    "avoid mis-classifications when the number of classes is more.\n",
    "Moreover, a dataset containing all food categories is also not\n",
    "available in the literature yet.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8228338\n",
    "<br>\n",
    "[2] https://keras.io/\n",
    "<br>\n",
    "[3] https://www.tensorflow.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
