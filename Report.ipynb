{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Food Classification from Images Using Convolutional Neural Networks</center>\n",
    "\n",
    "### <center>Course Project Report<center>\n",
    "###    <center>Submitted in partial fulfilment of the requirements for the<center>\n",
    "### <center>degree of<center>\n",
    "###    <center>Master of Technology in<center>\n",
    "### <center>Computer Science and Engineering<center>\n",
    "### <center>Under the guidance of<center>\n",
    "### <center>Nevil Anto<center>\n",
    "![Dataset](nitklogo.png)\n",
    "<div style=\"text-align: right\">\n",
    "\n",
    "\n",
    "<br>\n",
    "<b>172CS015 &nbsp; &nbsp; &nbsp;&nbsp;&nbsp; Kemanth PJ\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Contents</center>\n",
    "\n",
    "### Topic  ..................................................................................  Page no.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "### 1. Abstract ..................................................................................... 3\n",
    "<br>\n",
    "### 2. Introduction ............................................................................... 3\n",
    "<br>\n",
    "### 3. Related Work ............................................................................. 4\n",
    "<br>\n",
    "### 4. Overview .................................................................................... 6\n",
    "<br>\n",
    "### 5. Implementation ..........................................................................9\n",
    "<br>\n",
    "### 5. Experiments & Results .............................................................11\n",
    "<br>\n",
    "### 6. Conclusion .................................................................................14\n",
    "<br>\n",
    "### 7. References .................................................................................15\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Food Classification from Images Using Convolutional Neural Networks</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "Recently, smart applications for mobile devices such as Android phones and\n",
    "iPhone, have increased tremendously. Due to the advances invarious technologies\n",
    "used in smartphones, their computational power has also increased. In the current\n",
    "age, people are more conscious about their food and diet. We can use this\n",
    "technology to help people classify differnt types of food and their health benefits. In\n",
    "this paper, an approach has been presented to classify images of food using\n",
    "convolutional neural networks. Unlike the traditional artificial neural networks,\n",
    "convolutional neural networks have the capability of estimating the score function\n",
    "directly from image pixels. There are multiple such layers, and the outputs are\n",
    "concatenated at parts to form the final tensor of outputs. We use MAX pooling to\n",
    "extract essential features from the images and use it to train the model. An\n",
    "accuracy of 86.97% for the classes of the FOOD-101 dataset is recognised using\n",
    "the proposed implementation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "\n",
    "In the current age, people are more conscious about their\n",
    "food and diet to avoid either upcoming or existing diseases.\n",
    "Since people are dependent on smart technologies, provision\n",
    "of an application to automatically monitor the individuals diet,\n",
    "helps in many aspects. It increases the awareness of people in\n",
    "their food habits and diet.\n",
    "\n",
    "Recently, smart applications for mobile devices such as\n",
    "Android phones and iPhone, have increased tremendously.\n",
    "They are capable of balancing the food habits of users and\n",
    "also warn them about unhealthy food. Due to the advances in\n",
    "various technologies used in smartphones, their computational\n",
    "power has also increased. They are capable of processing\n",
    "real-time multi-media information with their computational\n",
    "power, whereas traditional mobiles are incapable and hence,\n",
    "used to send the images to high processing servers that\n",
    "increase the cost of communication and delay. Since the\n",
    "present smartphones can handle the high-quality images too,\n",
    "\n",
    "research on food classification is focused on developing\n",
    "real-time applications which capture images and train the\n",
    "machine learning models instantly. It helps to take prevention\n",
    "to avoids diseases such as diabetes, blood pressure and so on.\n",
    "Some of the methods currently in use for dietary assessment\n",
    "involve self-reporting and manually recorded instruments. The\n",
    "issue with such methods of assessment is that the evaluation of\n",
    "calorie consumption by a participant is prone to bias [6], i.e.\n",
    "underestimating and under reporting of food intake. In order to\n",
    "increase the accuracy and reduce the bias, enhancements to the\n",
    "current methods are required. One such potential solution is a\n",
    "mobile cloud computing system, which makes use of devices\n",
    "such as smartphones to capture dietary and calorie information.\n",
    "The next step is to automatically analyse the dietary and\n",
    "calorie information employing the computing capacity of the\n",
    "cloud for an objective assessment. However, users still have\n",
    "to enter the information manually. Over the last few years,\n",
    "plenty of research and development efforts have been made\n",
    "in the field of visual-based dietary and calorie information\n",
    "analysis. However, the efficient extraction of information from\n",
    "food images remains a challenging issue.\n",
    "In this paper, an effort has been made to classify the\n",
    "images of food for further diet monitoring applications using\n",
    "convolutional neural networks (CNNs). Since the CNNs are\n",
    "capable of handling a large amount of data and can estimate\n",
    "the features automatically, they have been utilised for the task\n",
    "of food classification. The standard Food-101 dataset has been\n",
    "selected as the working database for this approach.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Related Work\n",
    "\n",
    "<br/>\n",
    "<div style=\"text-align: justify\">\n",
    "The task of the food detection system is first initiated withfour  fast-food  classes  namely  fries,  apple  pies,  hamburgersand chicken burgers. The images were segmented initiallyto  form  the  feature  vector  with  size,  shape,  texture,  color(normalised  RGB),  and  other  context-based  features.  Withthis  motivation,  a  minimised  feature  vector  with  the  Gaborfilter responses (texture), pixel intensity, and color components is  used  to  categorise  the  19  classes  of  foods.  However,  theperformance  is  good  for  food  replicas,  and  a  less  efficientperformance  is  observed  with  real  images.  The  size  ofimages  and  their  variations  in  capturing  could  be  the  reasonfor the performance degradation. Based on this, scale invariantfeature  transform  (SIFT)  features  have  been  extracted  andexperimented  on  homemade  foods,  fast-food,  and  fruits  .With  this,  the  better  performance  is  found  with  less  numberof classes, although the images of each class are more.\n",
    "<br/>\n",
    "The term  bag  of features  (BoF) which  is derived  from thebag  of  words  (BoW)  is  the  emerging  trend  in  recent  days.It  is  highly  influenced  to  process  the  natural  language.  Itis  designed  to  catch  frequently  appearing  words  by  ignoringthe  order  in  which  they  appear.  Similarly,  imagescontain   some   common   visual   patterns   that   are   useful   inrecognising   the   category   of   food.   This   process   reducesthe  complexity  issues  raised  by  the  direct  image  matchingtechniques.  Based  on  this,  some  works  are  found  using  theBoF approach.\n",
    "<br/>\n",
    "Deep  Convolutional  Neural  Networks  have  been  used  forfood recognition recently, which have used the UEC-100and   UEC-256   datasets   for   testing,   along   with   ImageNetand   ILSVRC   for   training,   which   use   a   combination   ofbaseline  feature  extraction  and  neural  network  fine-tuning.Another  approach uses  Convolutional  Neural  Networksalong  with  a  Global  Average  Pooling  layer,  which  generatesFood  Activation  Maps  (heat  maps  of  food  probability).  Finetuning is done for FAM generation, which includes adding aconvolutional  layer  with  stride,  and  setting  a  softmax  layer.Additionally, via thresholding, bounding boxes are generated.The  present  work  aims  to  combine  some  of  the  abovemethodologies   together,   that   creates   a   food   classificationsystem, that predicts the class of food the image is in, and alsogives the calorie count based on the portion size visible. Thisconcept has a high scope in the health sector, as people want tokeep track of what and how much they eat and simplifying theprocess into the form of this implementation increases usageand awareness of health-related factors. Since CNNs are lessfocused in the literature, they have been utilized due to theirinherent capabilities in computing features automatically\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Food Classification using keras\n",
    "<br/>\n",
    "<div style=\"text-align: justify\">\n",
    "Convolutional neural networks (CNN) have been widely used in automatic image classification systems. In most cases, features from the top layer of the CNN are utilized for classification; however, those features may not contain enough useful information to predict an image correctly. In some cases, features from the lower layer carry more discriminative power than those from the top. Therefore, applying features from a specific layer only to classification seems to be a process that does not utilize learned CNN’s potential discriminant power to its full extent. This inherent property leads to the need for fusion of features from multiple layers. To address this problem, we propose a method of combining features from multiple layers in given CNN models. Moreover, already learned CNN models with training images are reused to extract features from multiple layers. The proposed fusion method is evaluated according to image classification benchmark data sets, CIFAR-10, NORB, and SVHN. In all cases, we show that the proposed method improves the reported performances of the existing models by 0.38%, 3.22% and 0.13%, respectively.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Convolutional Neural Networks\n",
    "<br/>\n",
    "<div style=\"text-align: justify\">\n",
    "The Convolutional neural network(CNN) is a deep learning architecture that has numerous application in computer vision and natural language processing. The CNN classifies objects based on number of features matched.\n",
    "\n",
    "Steps involed in creating CNN\n",
    "    <ol>\n",
    "<li>Convolution</li>\n",
    "<li> Pooling</li>\n",
    "<li> Flattening</li>\n",
    "<li> Full Connection</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![Dataset](convo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li><b>Convolution</b>\n",
    "<br/>\n",
    "<div style=\"text-align: justify\">\n",
    "ConvNets derive their name from the “convolution” operator. The primary purpose of Convolution in case of a ConvNet is to extract features from the input image. Convolution preserves the spatial relationship between pixels by learning image features using small squares of input data. \n",
    "</div>\n",
    "    </li>\n",
    "<li> <b>Pooling</b>\n",
    "<br/>\n",
    "<div style=\"text-align: justify\">\n",
    "Spatial Pooling (also called subsampling or downsampling) reduces the dimensionality of each feature map but retains the most important information. Spatial Pooling can be of different types: Max, Average, Sum etc.\n",
    "In case of Max Pooling, we define a spatial neighborhood (for example, a 2×2 window) and take the largest element from the rectified feature map within that window. Instead of taking the largest element we could also take the average (Average Pooling) or sum of all elements in that window. In practice, Max Pooling has been shown to work better.\n",
    "</div>\n",
    "    </li>\n",
    "<li> <b>Flattening</b>\n",
    "<br/>\n",
    "<div style=\"text-align: justify\">\n",
    "Convert the 2D matrix to a column vector so that it can passed through an artificial neural network\n",
    "</div>\n",
    "    </li>\n",
    "<li> <b>Full Connection</b>\n",
    "<br/>\n",
    "<div style=\"text-align: justify\">\n",
    "The Fully Connected layer is a traditional Multi Layer Perceptron that uses a softmax activation function in the output layer (other classifiers like SVM can also be used, but will stick to softmax in this post). The term “Fully Connected” implies that every neuron in the previous layer is connected to every neuron on the next layer.\n",
    "The output from the convolutional and pooling layers represent high-level features of the input image. The purpose of the Fully Connected layer is to use these features for classifying the input image into various classes based on the training dataset. \n",
    "</div>\n",
    "    </li>\n",
    "   </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementation of Food Classification using keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to import the libraries required by the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kemanth/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is located in folder named dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists of three classes, french_fries, pizza and samosa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"dataset.png\" alt = \"DataSet\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each folder contains approximately 1000 image files in each category. The name of the folder is actually the label of those files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the CNN\n",
    "classifier = Sequential()\n",
    "#Convolution and Max pooling\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (128, 128, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "classifier.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "classifier.add(Conv2D(128, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flatten\n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full connection\n",
    "classifier.add(Dense(128, activation = 'relu'))\n",
    "classifier.add(Dense(3, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile classifier\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2400 images belonging to 3 classes.\n",
      "Found 600 images belonging to 3 classes.\n",
      "Epoch 1/50\n",
      "25/25 [==============================] - 18s 719ms/step - loss: 1.1091 - acc: 0.4037 - val_loss: 1.0980 - val_acc: 0.3973\n",
      "Epoch 2/50\n",
      "25/25 [==============================] - 11s 447ms/step - loss: 1.0200 - acc: 0.4575 - val_loss: 1.1108 - val_acc: 0.4241\n",
      "Epoch 3/50\n",
      "25/25 [==============================] - 12s 493ms/step - loss: 1.0400 - acc: 0.4488 - val_loss: 0.9800 - val_acc: 0.4866\n",
      "Epoch 4/50\n",
      "25/25 [==============================] - 12s 481ms/step - loss: 0.9603 - acc: 0.5463 - val_loss: 1.0541 - val_acc: 0.5089\n",
      "Epoch 5/50\n",
      "25/25 [==============================] - 12s 461ms/step - loss: 0.9561 - acc: 0.5250 - val_loss: 0.8066 - val_acc: 0.6205\n",
      "Epoch 6/50\n",
      "25/25 [==============================] - 12s 476ms/step - loss: 0.8788 - acc: 0.6025 - val_loss: 0.8097 - val_acc: 0.6205\n",
      "Epoch 7/50\n",
      "25/25 [==============================] - 12s 500ms/step - loss: 0.8216 - acc: 0.6225 - val_loss: 0.8163 - val_acc: 0.6161\n",
      "Epoch 8/50\n",
      "25/25 [==============================] - 12s 473ms/step - loss: 0.7786 - acc: 0.6350 - val_loss: 0.6972 - val_acc: 0.6562\n",
      "Epoch 9/50\n",
      "25/25 [==============================] - 11s 457ms/step - loss: 0.7695 - acc: 0.6500 - val_loss: 0.7043 - val_acc: 0.6786\n",
      "Epoch 10/50\n",
      "25/25 [==============================] - 13s 501ms/step - loss: 0.7266 - acc: 0.6775 - val_loss: 0.7376 - val_acc: 0.6607\n",
      "Epoch 11/50\n",
      "25/25 [==============================] - 13s 513ms/step - loss: 0.7748 - acc: 0.6475 - val_loss: 0.9146 - val_acc: 0.5893\n",
      "Epoch 12/50\n",
      "25/25 [==============================] - 13s 520ms/step - loss: 0.6799 - acc: 0.6863 - val_loss: 0.6538 - val_acc: 0.7232\n",
      "Epoch 13/50\n",
      "25/25 [==============================] - 11s 459ms/step - loss: 0.6546 - acc: 0.7338 - val_loss: 0.7369 - val_acc: 0.6473\n",
      "Epoch 14/50\n",
      "25/25 [==============================] - 12s 471ms/step - loss: 0.6600 - acc: 0.7013 - val_loss: 0.6429 - val_acc: 0.7054\n",
      "Epoch 15/50\n",
      "25/25 [==============================] - 12s 494ms/step - loss: 0.6295 - acc: 0.7188 - val_loss: 0.6063 - val_acc: 0.7366\n",
      "Epoch 16/50\n",
      "25/25 [==============================] - 11s 447ms/step - loss: 0.6370 - acc: 0.7350 - val_loss: 0.7582 - val_acc: 0.6964\n",
      "Epoch 17/50\n",
      "25/25 [==============================] - 14s 552ms/step - loss: 0.5700 - acc: 0.7550 - val_loss: 0.6114 - val_acc: 0.7321\n",
      "Epoch 18/50\n",
      "25/25 [==============================] - 13s 510ms/step - loss: 0.5447 - acc: 0.7688 - val_loss: 0.7211 - val_acc: 0.7143\n",
      "Epoch 19/50\n",
      "25/25 [==============================] - 11s 428ms/step - loss: 0.6801 - acc: 0.7150 - val_loss: 0.6793 - val_acc: 0.7143\n",
      "Epoch 20/50\n",
      "25/25 [==============================] - 11s 438ms/step - loss: 0.5516 - acc: 0.7800 - val_loss: 0.6810 - val_acc: 0.7054\n",
      "Epoch 21/50\n",
      "25/25 [==============================] - 11s 448ms/step - loss: 0.6138 - acc: 0.7475 - val_loss: 0.7009 - val_acc: 0.6875\n",
      "Epoch 22/50\n",
      "25/25 [==============================] - 12s 467ms/step - loss: 0.5431 - acc: 0.7787 - val_loss: 0.5142 - val_acc: 0.7902\n",
      "Epoch 23/50\n",
      "25/25 [==============================] - 12s 498ms/step - loss: 0.5452 - acc: 0.7612 - val_loss: 0.5260 - val_acc: 0.7902\n",
      "Epoch 24/50\n",
      "25/25 [==============================] - 12s 495ms/step - loss: 0.5072 - acc: 0.7787 - val_loss: 0.5369 - val_acc: 0.8170\n",
      "Epoch 25/50\n",
      "25/25 [==============================] - 11s 442ms/step - loss: 0.4788 - acc: 0.7913 - val_loss: 0.9098 - val_acc: 0.6473\n",
      "Epoch 26/50\n",
      "25/25 [==============================] - 12s 496ms/step - loss: 0.4563 - acc: 0.8100 - val_loss: 0.5542 - val_acc: 0.7812\n",
      "Epoch 27/50\n",
      "25/25 [==============================] - 12s 469ms/step - loss: 0.4425 - acc: 0.8150 - val_loss: 0.4782 - val_acc: 0.8170\n",
      "Epoch 28/50\n",
      "25/25 [==============================] - 12s 491ms/step - loss: 0.3981 - acc: 0.8362 - val_loss: 0.9112 - val_acc: 0.6830\n",
      "Epoch 29/50\n",
      "25/25 [==============================] - 13s 511ms/step - loss: 0.5094 - acc: 0.7863 - val_loss: 0.5242 - val_acc: 0.7589\n",
      "Epoch 30/50\n",
      "25/25 [==============================] - 11s 456ms/step - loss: 0.4165 - acc: 0.8300 - val_loss: 0.5117 - val_acc: 0.8125\n",
      "Epoch 31/50\n",
      "25/25 [==============================] - 11s 435ms/step - loss: 0.4079 - acc: 0.8163 - val_loss: 0.4806 - val_acc: 0.8036\n",
      "Epoch 32/50\n",
      "25/25 [==============================] - 13s 501ms/step - loss: 0.3899 - acc: 0.8387 - val_loss: 0.6047 - val_acc: 0.7723\n",
      "Epoch 33/50\n",
      "25/25 [==============================] - 16s 621ms/step - loss: 0.4007 - acc: 0.8463 - val_loss: 0.4995 - val_acc: 0.8304\n",
      "Epoch 34/50\n",
      "25/25 [==============================] - 14s 548ms/step - loss: 0.3536 - acc: 0.8712 - val_loss: 0.4643 - val_acc: 0.8170\n",
      "Epoch 35/50\n",
      "25/25 [==============================] - 12s 500ms/step - loss: 0.3691 - acc: 0.8425 - val_loss: 0.6018 - val_acc: 0.7812\n",
      "Epoch 36/50\n",
      "25/25 [==============================] - 12s 498ms/step - loss: 0.3611 - acc: 0.8513 - val_loss: 0.5209 - val_acc: 0.8259\n",
      "Epoch 37/50\n",
      "25/25 [==============================] - 12s 463ms/step - loss: 0.3845 - acc: 0.8550 - val_loss: 0.5002 - val_acc: 0.7812\n",
      "Epoch 38/50\n",
      "25/25 [==============================] - 12s 485ms/step - loss: 0.3795 - acc: 0.8550 - val_loss: 0.6675 - val_acc: 0.7500\n",
      "Epoch 39/50\n",
      "25/25 [==============================] - 11s 440ms/step - loss: 0.3756 - acc: 0.8500 - val_loss: 0.4993 - val_acc: 0.7946\n",
      "Epoch 40/50\n",
      "25/25 [==============================] - 11s 429ms/step - loss: 0.2644 - acc: 0.9050 - val_loss: 0.4811 - val_acc: 0.8304\n",
      "Epoch 41/50\n",
      "25/25 [==============================] - 12s 470ms/step - loss: 0.3743 - acc: 0.8513 - val_loss: 0.4262 - val_acc: 0.8393\n",
      "Epoch 42/50\n",
      "25/25 [==============================] - 12s 471ms/step - loss: 0.3691 - acc: 0.8575 - val_loss: 0.4909 - val_acc: 0.8214\n",
      "Epoch 43/50\n",
      "25/25 [==============================] - 11s 435ms/step - loss: 0.3402 - acc: 0.8712 - val_loss: 0.4946 - val_acc: 0.8170\n",
      "Epoch 44/50\n",
      "25/25 [==============================] - 12s 461ms/step - loss: 0.3293 - acc: 0.8837 - val_loss: 0.5053 - val_acc: 0.7902\n",
      "Epoch 45/50\n",
      "25/25 [==============================] - 11s 450ms/step - loss: 0.2964 - acc: 0.8925 - val_loss: 0.4128 - val_acc: 0.8393\n",
      "Epoch 46/50\n",
      "25/25 [==============================] - 12s 466ms/step - loss: 0.2298 - acc: 0.9062 - val_loss: 0.5933 - val_acc: 0.8125\n",
      "Epoch 47/50\n",
      "25/25 [==============================] - 12s 480ms/step - loss: 0.2714 - acc: 0.9012 - val_loss: 0.3832 - val_acc: 0.8616\n",
      "Epoch 48/50\n",
      "25/25 [==============================] - 12s 464ms/step - loss: 0.2497 - acc: 0.9025 - val_loss: 0.5509 - val_acc: 0.8304\n",
      "Epoch 49/50\n",
      "25/25 [==============================] - 12s 460ms/step - loss: 0.2547 - acc: 0.9088 - val_loss: 0.6035 - val_acc: 0.7812\n",
      "Epoch 50/50\n",
      "25/25 [==============================] - 12s 467ms/step - loss: 0.2570 - acc: 0.8962 - val_loss: 0.3439 - val_acc: 0.9196\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc03aee2c18>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting CNN to the images\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True) \n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "training_set = train_datagen.flow_from_directory('./dataset/training_set', target_size=(128, 128), batch_size=32, class_mode='categorical')\n",
    "test_set = test_datagen.flow_from_directory('./dataset/test_set', target_size=(128, 128), batch_size=32, class_mode='categorical')\n",
    "classifier.fit_generator(training_set, steps_per_epoch=800/32, epochs=50, validation_data=test_set, validation_steps = 200/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "import os\n",
    "target_dir = './models/'\n",
    "if not os.path.exists(target_dir):\n",
    "  os.mkdir(target_dir)\n",
    "classifier.save('./models/model.h5')\n",
    "classifier.save_weights('./models/weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Creating a GUI and predicting a new image from url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries for creating GUI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from keras.models import Sequential, load_model\n",
    "from PIL import Image, ImageTk\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from tkinter import Tk,Label,Canvas,NW,Entry,Button \n",
    "from keras.preprocessing import image as image_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"justify\">\n",
    "    We now create a url and a button for browsing and loading image.\n",
    "The trained CNN is loaded and the saved parameters are restored.\n",
    "Prediction is made on the new image which is then output and \n",
    "the output is displayed on canvas.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "img_width, img_height = 128, 128\n",
    "model_path = './models/model.h5'\n",
    "model_weights_path = './models/weights.h5'\n",
    "model = load_model(model_path)\n",
    "model.load_weights(model_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main code which creates the gui with the help of clicked button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = ''\n",
    "window = Tk()\n",
    "window.title(\"Welcome to Image predictor\") \n",
    "window.geometry('800x600')\n",
    "lbl = Label(window, text=\"Enter the URL of the image\", font=(\"Helvetica\", 16))\n",
    "lbl.pack()\n",
    "def clicked(): \n",
    "    global url\n",
    "    lbl.configure()\n",
    "    url  = (User_input.get())\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    test_image = Image.open(BytesIO(response.content))\n",
    "    put_image = test_image.resize((400,400)) \n",
    "    test_image = test_image.resize((128,128))  \n",
    "    img = ImageTk.PhotoImage(put_image)\n",
    "    pic = Label(image=img)\n",
    "    pic.pack()\n",
    "    pic.image = img\n",
    "    test_image = image_utils.img_to_array(test_image)\n",
    "    test_image = np.expand_dims(test_image, axis=0)\n",
    "    \n",
    " \n",
    "    result = model.predict_on_batch(test_image)\n",
    "\n",
    "    if result[0][0] == 1:\n",
    "        ans = 'french fries'\n",
    "    elif result[0][1] == 1:\n",
    "        ans = 'pizza'\n",
    "    elif result[0][2] == 1:\n",
    "        ans = 'samosa'\n",
    "    out = Label(window, text  = 'Predicted answer : ' +  ans, font=(\"Helvetica\", 16))\n",
    "    out.pack()\n",
    "\n",
    "User_input = Entry(width = 100)\n",
    "User_input.pack()\n",
    "btn = Button(window, text=\"Detect Image\", font=(\"Helvetica\", 12), command=clicked)\n",
    "btn.pack()\n",
    "window.mainloop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 A sample run "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>The input window</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"gui1.png\" height= 30% width = 40%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Predicting Output </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"gui2.png\" width = 40% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<div style=\"text-align: justify\">\n",
    "The performance of the system is high, and is considered\n",
    "acceptable from a usage point of view. However, the CNNs\n",
    "need high-performance computing machines in order to\n",
    "experiment on the huge multi-media datasets. The CNN\n",
    "is capable of train highly non-linear data, and for that in contrast, it takes more computational time to train the network.\n",
    "However, the performance matters a lot, and once the system\n",
    "is properly trained, the system can produce the results in less\n",
    "time. The images are properly preprocessed and all kinds of\n",
    "images are tested with CNN. From this, it is concluded that\n",
    "CNNs are more suitable for classifying the images when the\n",
    "number of classes are more.\n",
    "The task of image classification can be extended using\n",
    "prominent features that can categorize food images. Since\n",
    "the CNNs are consuming high computational time, the\n",
    "feature-based approach is highly appreciable. A multi-level\n",
    "classification approach (hierarchical approach) is suitable to\n",
    "avoid mis-classifications when the number of classes is more.\n",
    "Moreover, a dataset containing all food categories is also not\n",
    "available in the literature yet.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8228338\n",
    "<br>\n",
    "[2] https://keras.io/\n",
    "<br>\n",
    "[3] https://www.tensorflow.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
