{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Food Classification from Images Using Convolutional Neural Networks</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Food Classification using keras\n",
    "<br/>\n",
    "<div style=\"text-align: justify\">\n",
    "Convolutional neural networks (CNN) have been widely used in automatic image classification systems. In most cases, features from the top layer of the CNN are utilized for classification; however, those features may not contain enough useful information to predict an image correctly. In some cases, features from the lower layer carry more discriminative power than those from the top. Therefore, applying features from a specific layer only to classification seems to be a process that does not utilize learned CNN’s potential discriminant power to its full extent. This inherent property leads to the need for fusion of features from multiple layers. To address this problem, we propose a method of combining features from multiple layers in given CNN models. Moreover, already learned CNN models with training images are reused to extract features from multiple layers. The proposed fusion method is evaluated according to image classification benchmark data sets, CIFAR-10, NORB, and SVHN. In all cases, we show that the proposed method improves the reported performances of the existing models by 0.38%, 3.22% and 0.13%, respectively.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks\n",
    "<br/>\n",
    "<div style=\"text-align: justify\">\n",
    "The Convolutional neural network(CNN) is a deep learning architecture that has numerous application in computer vision and natural language processing. The CNN classifies objects based on number of features matched.\n",
    "\n",
    "Steps involed in creating CNN\n",
    "    <ol>\n",
    "<li>Convolution</li>\n",
    "<li> Pooling</li>\n",
    "<li> Flattening</li>\n",
    "<li> Full Connection</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![Dataset](convo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li><b>Convolution</b>\n",
    "<br/>\n",
    "<div style=\"text-align: justify\">\n",
    "ConvNets derive their name from the “convolution” operator. The primary purpose of Convolution in case of a ConvNet is to extract features from the input image. Convolution preserves the spatial relationship between pixels by learning image features using small squares of input data. \n",
    "</div>\n",
    "    </li>\n",
    "<li> <b>Pooling</b>\n",
    "<br/>\n",
    "<div style=\"text-align: justify\">\n",
    "Spatial Pooling (also called subsampling or downsampling) reduces the dimensionality of each feature map but retains the most important information. Spatial Pooling can be of different types: Max, Average, Sum etc.\n",
    "In case of Max Pooling, we define a spatial neighborhood (for example, a 2×2 window) and take the largest element from the rectified feature map within that window. Instead of taking the largest element we could also take the average (Average Pooling) or sum of all elements in that window. In practice, Max Pooling has been shown to work better.\n",
    "</div>\n",
    "    </li>\n",
    "<li> <b>Flattening</b>\n",
    "<br/>\n",
    "<div style=\"text-align: justify\">\n",
    "Convert the 2D matrix to a column vector so that it can passed through an artificial neural network\n",
    "</div>\n",
    "    </li>\n",
    "<li> <b>Full Connection</b>\n",
    "<br/>\n",
    "<div style=\"text-align: justify\">\n",
    "The Fully Connected layer is a traditional Multi Layer Perceptron that uses a softmax activation function in the output layer (other classifiers like SVM can also be used, but will stick to softmax in this post). The term “Fully Connected” implies that every neuron in the previous layer is connected to every neuron on the next layer.\n",
    "The output from the convolutional and pooling layers represent high-level features of the input image. The purpose of the Fully Connected layer is to use these features for classifying the input image into various classes based on the training dataset. \n",
    "</div>\n",
    "    </li>\n",
    "   </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Food Classification using keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to import the libraries required by the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is located in folder named dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists of three classes, french_fries, pizza and samosa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"dataset.png\" alt = \"DataSet\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each folder contains approximately 1000 image files in each category. The name of the folder is actually the label of those files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Initialize the CNN\n",
    "classifier = Sequential()\n",
    "#Convolution and Max pooling\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (128, 128, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "classifier.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "classifier.add(Conv2D(128, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flatten\n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full connection\n",
    "classifier.add(Dense(128, activation = 'relu'))\n",
    "classifier.add(Dense(3, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile classifier\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2400 images belonging to 3 classes.\n",
      "Found 600 images belonging to 3 classes.\n",
      "Epoch 1/50\n",
      "25/25 [==============================] - 32s 1s/step - loss: 0.2150 - acc: 0.9225 - val_loss: 0.5462 - val_acc: 0.8229\n",
      "Epoch 2/50\n",
      "25/25 [==============================] - 28s 1s/step - loss: 0.2200 - acc: 0.9138 - val_loss: 0.4753 - val_acc: 0.8073\n",
      "Epoch 3/50\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.2082 - acc: 0.9175 - val_loss: 0.3808 - val_acc: 0.8698\n",
      "Epoch 4/50\n",
      "25/25 [==============================] - 30s 1s/step - loss: 0.1807 - acc: 0.9413 - val_loss: 0.4416 - val_acc: 0.8478\n",
      "Epoch 5/50\n",
      "25/25 [==============================] - 37s 1s/step - loss: 0.2614 - acc: 0.9050 - val_loss: 0.3370 - val_acc: 0.8698\n",
      "Epoch 6/50\n",
      "25/25 [==============================] - 30s 1s/step - loss: 0.1979 - acc: 0.9275 - val_loss: 0.4043 - val_acc: 0.8906\n",
      "Epoch 7/50\n",
      "25/25 [==============================] - 31s 1s/step - loss: 0.2015 - acc: 0.9175 - val_loss: 0.3865 - val_acc: 0.8967\n",
      "Epoch 8/50\n",
      "25/25 [==============================] - 36s 1s/step - loss: 0.1799 - acc: 0.9250 - val_loss: 0.3249 - val_acc: 0.8646\n",
      "Epoch 9/50\n",
      "25/25 [==============================] - 35s 1s/step - loss: 0.1605 - acc: 0.9387 - val_loss: 0.3900 - val_acc: 0.8802\n",
      "Epoch 10/50\n",
      "25/25 [==============================] - 34s 1s/step - loss: 0.1574 - acc: 0.9500 - val_loss: 0.4541 - val_acc: 0.8641\n",
      "Epoch 11/50\n",
      "25/25 [==============================] - 40s 2s/step - loss: 0.1966 - acc: 0.9300 - val_loss: 0.4459 - val_acc: 0.8385\n",
      "Epoch 12/50\n",
      "25/25 [==============================] - 35s 1s/step - loss: 0.1692 - acc: 0.9425 - val_loss: 0.4483 - val_acc: 0.8438\n",
      "Epoch 13/50\n",
      "25/25 [==============================] - 35s 1s/step - loss: 0.1714 - acc: 0.9350 - val_loss: 0.5203 - val_acc: 0.8478\n",
      "Epoch 14/50\n",
      "25/25 [==============================] - 36s 1s/step - loss: 0.1814 - acc: 0.9400 - val_loss: 0.3164 - val_acc: 0.8906\n",
      "Epoch 15/50\n",
      "25/25 [==============================] - 33s 1s/step - loss: 0.1766 - acc: 0.9387 - val_loss: 0.5210 - val_acc: 0.8177\n",
      "Epoch 16/50\n",
      "25/25 [==============================] - 36s 1s/step - loss: 0.1215 - acc: 0.9575 - val_loss: 0.3863 - val_acc: 0.8641\n",
      "Epoch 17/50\n",
      "25/25 [==============================] - 34s 1s/step - loss: 0.1536 - acc: 0.9425 - val_loss: 0.3950 - val_acc: 0.8854\n",
      "Epoch 18/50\n",
      "25/25 [==============================] - 35s 1s/step - loss: 0.1844 - acc: 0.9337 - val_loss: 0.4240 - val_acc: 0.8802\n",
      "Epoch 19/50\n",
      "25/25 [==============================] - 36s 1s/step - loss: 0.1547 - acc: 0.9475 - val_loss: 0.3173 - val_acc: 0.8750\n",
      "Epoch 20/50\n",
      "25/25 [==============================] - 41s 2s/step - loss: 0.1817 - acc: 0.9288 - val_loss: 0.5289 - val_acc: 0.8594\n",
      "Epoch 21/50\n",
      "25/25 [==============================] - 35s 1s/step - loss: 0.1648 - acc: 0.9475 - val_loss: 0.5513 - val_acc: 0.8177\n",
      "Epoch 22/50\n",
      "25/25 [==============================] - 38s 2s/step - loss: 0.1238 - acc: 0.9525 - val_loss: 0.4217 - val_acc: 0.8750\n",
      "Epoch 23/50\n",
      "25/25 [==============================] - 36s 1s/step - loss: 0.1329 - acc: 0.9513 - val_loss: 0.2870 - val_acc: 0.9022\n",
      "Epoch 24/50\n",
      "25/25 [==============================] - 37s 1s/step - loss: 0.1516 - acc: 0.9500 - val_loss: 0.4779 - val_acc: 0.8490\n",
      "Epoch 25/50\n",
      "25/25 [==============================] - 35s 1s/step - loss: 0.1108 - acc: 0.9688 - val_loss: 0.4424 - val_acc: 0.8646\n",
      "Epoch 26/50\n",
      "25/25 [==============================] - 32s 1s/step - loss: 0.1400 - acc: 0.9563 - val_loss: 0.4365 - val_acc: 0.8859\n",
      "Epoch 27/50\n",
      "25/25 [==============================] - 38s 2s/step - loss: 0.1136 - acc: 0.9550 - val_loss: 0.6662 - val_acc: 0.7917\n",
      "Epoch 28/50\n",
      "25/25 [==============================] - 35s 1s/step - loss: 0.1128 - acc: 0.9675 - val_loss: 0.2610 - val_acc: 0.9167\n",
      "Epoch 29/50\n",
      "25/25 [==============================] - 37s 1s/step - loss: 0.0954 - acc: 0.9675 - val_loss: 0.4144 - val_acc: 0.8913\n",
      "Epoch 30/50\n",
      "25/25 [==============================] - 39s 2s/step - loss: 0.1005 - acc: 0.9688 - val_loss: 0.5769 - val_acc: 0.8333\n",
      "Epoch 31/50\n",
      "25/25 [==============================] - 35s 1s/step - loss: 0.0785 - acc: 0.9763 - val_loss: 0.2908 - val_acc: 0.9219\n",
      "Epoch 32/50\n",
      "25/25 [==============================] - 33s 1s/step - loss: 0.0903 - acc: 0.9725 - val_loss: 0.3573 - val_acc: 0.8804\n",
      "Epoch 33/50\n",
      "25/25 [==============================] - 35s 1s/step - loss: 0.1043 - acc: 0.9625 - val_loss: 0.3816 - val_acc: 0.8802\n",
      "Epoch 34/50\n",
      "25/25 [==============================] - 38s 2s/step - loss: 0.1120 - acc: 0.9537 - val_loss: 0.3884 - val_acc: 0.9010\n",
      "Epoch 35/50\n",
      "25/25 [==============================] - 37s 1s/step - loss: 0.1146 - acc: 0.9587 - val_loss: 0.5652 - val_acc: 0.8370\n",
      "Epoch 36/50\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.1001 - acc: 0.9637 - val_loss: 0.3612 - val_acc: 0.8750\n",
      "Epoch 37/50\n",
      "25/25 [==============================] - 23s 939ms/step - loss: 0.0844 - acc: 0.9725 - val_loss: 0.4775 - val_acc: 0.8646\n",
      "Epoch 38/50\n",
      "25/25 [==============================] - 32s 1s/step - loss: 0.1063 - acc: 0.9612 - val_loss: 0.5218 - val_acc: 0.8696\n",
      "Epoch 39/50\n",
      "25/25 [==============================] - 28s 1s/step - loss: 0.1110 - acc: 0.9650 - val_loss: 0.3130 - val_acc: 0.8802\n",
      "Epoch 40/50\n",
      "25/25 [==============================] - 25s 981ms/step - loss: 0.0935 - acc: 0.9713 - val_loss: 0.3808 - val_acc: 0.8958\n",
      "Epoch 41/50\n",
      "25/25 [==============================] - 24s 948ms/step - loss: 0.0839 - acc: 0.9800 - val_loss: 0.4546 - val_acc: 0.8854\n",
      "Epoch 42/50\n",
      "25/25 [==============================] - 24s 970ms/step - loss: 0.0936 - acc: 0.9700 - val_loss: 0.3132 - val_acc: 0.8913\n",
      "Epoch 43/50\n",
      "25/25 [==============================] - 23s 901ms/step - loss: 0.0587 - acc: 0.9812 - val_loss: 0.2823 - val_acc: 0.9115\n",
      "Epoch 44/50\n",
      "25/25 [==============================] - 22s 887ms/step - loss: 0.0840 - acc: 0.9738 - val_loss: 0.5096 - val_acc: 0.8542\n",
      "Epoch 45/50\n",
      "25/25 [==============================] - 21s 845ms/step - loss: 0.0615 - acc: 0.9775 - val_loss: 0.4620 - val_acc: 0.8641\n",
      "Epoch 46/50\n",
      "25/25 [==============================] - 25s 983ms/step - loss: 0.0663 - acc: 0.9775 - val_loss: 0.6468 - val_acc: 0.8281\n",
      "Epoch 47/50\n",
      "25/25 [==============================] - 23s 918ms/step - loss: 0.0733 - acc: 0.9800 - val_loss: 0.4466 - val_acc: 0.8542\n",
      "Epoch 48/50\n",
      "25/25 [==============================] - 26s 1s/step - loss: 0.1409 - acc: 0.9525 - val_loss: 0.3722 - val_acc: 0.8967\n",
      "Epoch 49/50\n",
      "25/25 [==============================] - 29s 1s/step - loss: 0.0845 - acc: 0.9625 - val_loss: 0.4999 - val_acc: 0.9010\n",
      "Epoch 50/50\n",
      "25/25 [==============================] - 24s 972ms/step - loss: 0.0782 - acc: 0.9788 - val_loss: 0.4549 - val_acc: 0.8698\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb33b65ef0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting CNN to the images\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True) \n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "training_set = train_datagen.flow_from_directory('./dataset/training_set', target_size=(128, 128), batch_size=32, class_mode='categorical')\n",
    "test_set = test_datagen.flow_from_directory('./dataset/test_set', target_size=(128, 128), batch_size=32, class_mode='categorical')\n",
    "STEP_SIZE_TRAIN=800//training_set.batch_size\n",
    "STEP_SIZE_TEST=200//test_set.batch_size\n",
    "classifier.fit_generator(training_set, steps_per_epoch=STEP_SIZE_TRAIN, epochs=50, validation_data=test_set, validation_steps = STEP_SIZE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4245157341162364, 0.8854166666666666]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluating the model\n",
    "classifier.evaluate_generator(generator=test_set,\n",
    "steps=STEP_SIZE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'french_fries', 1: 'pizza', 2: 'samosa'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = (training_set.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "import os\n",
    "target_dir = './models/'\n",
    "if not os.path.exists(target_dir):\n",
    "  os.mkdir(target_dir)\n",
    "classifier.save('./models/model.h5')\n",
    "classifier.save_weights('./models/weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting a new image from url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries for creating GUI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from keras.models import Sequential, load_model\n",
    "from PIL import Image, ImageTk\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from keras.preprocessing import image as image_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"justify\">\n",
    "    We now create a url and a button for browsing and loading image.\n",
    "The trained CNN is loaded and the saved parameters are restored.\n",
    "Prediction is made on the new image which is then output and \n",
    "the output is displayed on canvas.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "img_width, img_height = 128, 128\n",
    "model_path = './models/model.h5'\n",
    "model_weights_path = './models/weights.h5'\n",
    "model = load_model(model_path)\n",
    "model.load_weights(model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.kingarthurflour.com/sites/default/files/recipe_legacy/20-3-large.jpg\n",
      "Predicted answer : \n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.kingarthurflour.com/sites/default/files/recipe_legacy/20-3-large.jpg' # change to image url\n",
    "print(url)\n",
    "response = requests.get(url)\n",
    "test_image = Image.open(BytesIO(response.content))\n",
    "put_image = test_image.resize((400,400)) \n",
    "test_image = test_image.resize((128,128))  \n",
    "test_image = image_utils.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "result = model.predict_on_batch(test_image)\n",
    "\n",
    "if result[0][0] == 1:\n",
    "    ans = 'french fries'\n",
    "elif result[0][1] == 1:\n",
    "    ans = 'pizza'\n",
    "elif result[0][2] == 1:\n",
    "    ans = 'samosa'\n",
    "    \n",
    "print('Predicted answer : ' +  ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<div style=\"text-align: justify\">\n",
    "The performance of the system is high, and is considered\n",
    "acceptable from a usage point of view. However, the CNNs\n",
    "need high-performance computing machines in order to\n",
    "experiment on the huge multi-media datasets. The CNN\n",
    "is capable of train highly non-linear data, and for that in contrast, it takes more computational time to train the network.\n",
    "However, the performance matters a lot, and once the system\n",
    "is properly trained, the system can produce the results in less\n",
    "time. The images are properly preprocessed and all kinds of\n",
    "images are tested with CNN. From this, it is concluded that\n",
    "CNNs are more suitable for classifying the images when the\n",
    "number of classes are more.\n",
    "The task of image classification can be extended using\n",
    "prominent features that can categorize food images. Since\n",
    "the CNNs are consuming high computational time, the\n",
    "feature-based approach is highly appreciable. A multi-level\n",
    "classification approach (hierarchical approach) is suitable to\n",
    "avoid mis-classifications when the number of classes is more.\n",
    "Moreover, a dataset containing all food categories is also not\n",
    "available in the literature yet.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8228338\n",
    "<br>\n",
    "[2] https://keras.io/\n",
    "<br>\n",
    "[3] https://www.tensorflow.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
